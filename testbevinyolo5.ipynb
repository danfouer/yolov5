{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 导入所有依赖项"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T00:37:12.659358Z",
     "start_time": "2025-03-23T00:36:48.216833Z"
    }
   },
   "source": [
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import platform\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "#FILE = Path(__file__).resolve()\n",
    "ROOT = os.path.dirname(os.path.abspath('file'))\n",
    "\n",
    "#FILE=os.path.abspath('file')\n",
    "#print(ROOT)\n",
    "#ROOT = FILE.parents[0]  # YOLOv5 root directory\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
    "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n",
    "\n",
    "from ultralytics.utils.plotting import Annotator, colors, save_one_box\n",
    "\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\n",
    "from utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n",
    "                           increment_path, non_max_suppression, print_args, scale_boxes, strip_optimizer, xyxy2xywh)\n",
    "from utils.torch_utils import select_device, smart_inference_mode\n",
    "from utils.utilsbev import *\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T00:37:19.345271Z",
     "start_time": "2025-03-23T00:37:19.332307Z"
    }
   },
   "source": [
    "# import torch\n",
    "# import utils\n",
    "# display = utils.notebook_init()  # checks"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T00:37:19.972047Z",
     "start_time": "2025-03-23T00:37:19.965067Z"
    }
   },
   "source": [
    "#!python detect.py --weights ZZX12Class.pt --img 640 --conf 0.25 --source bevdata/caltech_washington1"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T00:38:46.677819Z",
     "start_time": "2025-03-23T00:38:46.455416Z"
    }
   },
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = select_device(device)\n",
    "#print(device)\n",
    "weights=ROOT / 'ZZX12Class.pt'\n",
    "source=ROOT / 'bevdata/caltech_washington1'\n",
    "data=ROOT / 'data/bdd100k.yaml',  # dataset.yaml path\n",
    "imgsz=(480, 640),  # inference size (height, width)\n",
    "model = DetectMultiBackend(weights,device=device,dnn=False, data=data, fp16=False)\n",
    "stride, names, pt = model.stride, model.names, model.pt\n",
    "print(stride, names, pt)\n",
    "imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  e1894300 Python-3.9.21 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1070, 8192MiB)\n",
      "\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001B[1mdo those steps only if you trust the source of the checkpoint\u001B[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnpicklingError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m data\u001B[38;5;241m=\u001B[39mROOT \u001B[38;5;241m/\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/bdd100k.yaml\u001B[39m\u001B[38;5;124m'\u001B[39m,  \u001B[38;5;66;03m# dataset.yaml path\u001B[39;00m\n\u001B[0;32m      7\u001B[0m imgsz\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m480\u001B[39m, \u001B[38;5;241m640\u001B[39m),  \u001B[38;5;66;03m# inference size (height, width)\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mDetectMultiBackend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweights\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdnn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfp16\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m stride, names, pt \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mstride, model\u001B[38;5;241m.\u001B[39mnames, model\u001B[38;5;241m.\u001B[39mpt\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(stride, names, pt)\n",
      "File \u001B[1;32mD:\\Pro\\Pycharm\\yolov5\\models\\common.py:356\u001B[0m, in \u001B[0;36mDetectMultiBackend.__init__\u001B[1;34m(self, weights, device, dnn, data, fp16, fuse)\u001B[0m\n\u001B[0;32m    353\u001B[0m     w \u001B[38;5;241m=\u001B[39m attempt_download(w)  \u001B[38;5;66;03m# download if not local\u001B[39;00m\n\u001B[0;32m    355\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pt:  \u001B[38;5;66;03m# PyTorch\u001B[39;00m\n\u001B[1;32m--> 356\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mattempt_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweights\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mweights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfuse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfuse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    357\u001B[0m     stride \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mint\u001B[39m(model\u001B[38;5;241m.\u001B[39mstride\u001B[38;5;241m.\u001B[39mmax()), \u001B[38;5;241m32\u001B[39m)  \u001B[38;5;66;03m# model stride\u001B[39;00m\n\u001B[0;32m    358\u001B[0m     names \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mmodule\u001B[38;5;241m.\u001B[39mnames \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(model, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodule\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m model\u001B[38;5;241m.\u001B[39mnames  \u001B[38;5;66;03m# get class names\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Pro\\Pycharm\\yolov5\\models\\experimental.py:79\u001B[0m, in \u001B[0;36mattempt_load\u001B[1;34m(weights, device, inplace, fuse)\u001B[0m\n\u001B[0;32m     77\u001B[0m model \u001B[38;5;241m=\u001B[39m Ensemble()\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m weights \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(weights, \u001B[38;5;28mlist\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [weights]:\n\u001B[1;32m---> 79\u001B[0m     ckpt \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattempt_download\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# load\u001B[39;00m\n\u001B[0;32m     80\u001B[0m     ckpt \u001B[38;5;241m=\u001B[39m (ckpt\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mema\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m ckpt[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mfloat()  \u001B[38;5;66;03m# FP32 model\u001B[39;00m\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;66;03m# Model compatibility updates\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Tools\\anaconda3\\envs\\RoboYolo\\lib\\site-packages\\torch\\serialization.py:1470\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[0;32m   1462\u001B[0m                 \u001B[38;5;28;01mreturn\u001B[39;00m _load(\n\u001B[0;32m   1463\u001B[0m                     opened_zipfile,\n\u001B[0;32m   1464\u001B[0m                     map_location,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1467\u001B[0m                     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpickle_load_args,\n\u001B[0;32m   1468\u001B[0m                 )\n\u001B[0;32m   1469\u001B[0m             \u001B[38;5;28;01mexcept\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mUnpicklingError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m-> 1470\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mUnpicklingError(_get_wo_message(\u001B[38;5;28mstr\u001B[39m(e))) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1471\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _load(\n\u001B[0;32m   1472\u001B[0m             opened_zipfile,\n\u001B[0;32m   1473\u001B[0m             map_location,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1476\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpickle_load_args,\n\u001B[0;32m   1477\u001B[0m         )\n\u001B[0;32m   1478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mmap:\n",
      "\u001B[1;31mUnpicklingError\u001B[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001B[1mdo those steps only if you trust the source of the checkpoint\u001B[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` or the `torch.serialization.safe_globals([_reconstruct])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T00:38:48.952349Z",
     "start_time": "2025-03-23T00:38:48.918440Z"
    }
   },
   "source": [
    "bs = 1  # batch_size\n",
    "dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=1)\n",
    "vid_path, vid_writer = [None] * bs, [None] * bs"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stride' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m bs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m  \u001B[38;5;66;03m# batch_size\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m dataset \u001B[38;5;241m=\u001B[39m LoadImages(source, img_size\u001B[38;5;241m=\u001B[39mimgsz, stride\u001B[38;5;241m=\u001B[39m\u001B[43mstride\u001B[49m, auto\u001B[38;5;241m=\u001B[39mpt, vid_stride\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      3\u001B[0m vid_path, vid_writer \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m*\u001B[39m bs, [\u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m*\u001B[39m bs\n",
      "\u001B[1;31mNameError\u001B[0m: name 'stride' is not defined"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty(): argument 'size' failed to unpack the object at pos 3 with error \"type must be tuple of ints,but got tuple\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32mf:\\VCStudio\\yolov5\\testbevinyolo5.ipynb Cell 10\u001B[0m line \u001B[0;36m1\n\u001B[1;32m----> <a href='vscode-notebook-cell:/f%3A/VCStudio/yolov5/testbevinyolo5.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001B[0m model\u001B[39m.\u001B[39;49mwarmup(imgsz\u001B[39m=\u001B[39;49m(\u001B[39m1\u001B[39;49m, \u001B[39m3\u001B[39;49m,\u001B[39m*\u001B[39;49mimgsz))  \u001B[39m# warmup\u001B[39;00m\n",
      "File \u001B[1;32mf:\\VCStudio\\yolov5\\models\\common.py:607\u001B[0m, in \u001B[0;36mDetectMultiBackend.warmup\u001B[1;34m(self, imgsz)\u001B[0m\n\u001B[0;32m    605\u001B[0m warmup_types \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mpt, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mjit, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39monnx, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mengine, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39msaved_model, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mpb, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mtriton\n\u001B[0;32m    606\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39many\u001B[39m(warmup_types) \u001B[39mand\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdevice\u001B[39m.\u001B[39mtype \u001B[39m!=\u001B[39m \u001B[39m'\u001B[39m\u001B[39mcpu\u001B[39m\u001B[39m'\u001B[39m \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mtriton):\n\u001B[1;32m--> 607\u001B[0m     im \u001B[39m=\u001B[39m torch\u001B[39m.\u001B[39;49mempty(\u001B[39m*\u001B[39;49mimgsz, dtype\u001B[39m=\u001B[39;49mtorch\u001B[39m.\u001B[39;49mhalf \u001B[39mif\u001B[39;49;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mfp16 \u001B[39melse\u001B[39;49;00m torch\u001B[39m.\u001B[39;49mfloat, device\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mdevice)  \u001B[39m# input\u001B[39;00m\n\u001B[0;32m    608\u001B[0m     \u001B[39mfor\u001B[39;00m _ \u001B[39min\u001B[39;00m \u001B[39mrange\u001B[39m(\u001B[39m2\u001B[39m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mjit \u001B[39melse\u001B[39;00m \u001B[39m1\u001B[39m):  \u001B[39m#\u001B[39;00m\n\u001B[0;32m    609\u001B[0m         \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mforward(im)\n",
      "\u001B[1;31mTypeError\u001B[0m: empty(): argument 'size' failed to unpack the object at pos 3 with error \"type must be tuple of ints,but got tuple\""
     ]
    }
   ],
   "source": [
    "model.warmup(imgsz=(1, 3,*imgsz))  # warmup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RoboYoloW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
