{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 导入所有依赖项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T06:02:31.986445Z",
     "start_time": "2023-11-06T06:02:31.855881400Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import platform\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "#FILE = Path(__file__).resolve()\n",
    "ROOT = os.path.dirname(os.path.abspath('file'))\n",
    "\n",
    "#FILE=os.path.abspath('file')\n",
    "#print(ROOT)\n",
    "#ROOT = FILE.parents[0]  # YOLOv5 root directory\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
    "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n",
    "\n",
    "from ultralytics.utils.plotting import Annotator, colors, save_one_box\n",
    "\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\n",
    "from utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n",
    "                           increment_path, non_max_suppression, print_args, scale_boxes, strip_optimizer, xyxy2xywh)\n",
    "from utils.torch_utils import select_device, smart_inference_mode\n",
    "from utils.utilsbev import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import utils\n",
    "# display = utils.notebook_init()  # checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python detect.py --weights ZZX12Class.pt --img 640 --conf 0.25 --source bevdata/caltech_washington1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T06:02:42.834319900Z",
     "start_time": "2023-11-06T06:02:40.691745100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  d5d514e Python-3.9.18 torch-2.1.0+cu118 CUDA:0 (Quadro RTX 4000, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 232 layers, 7276185 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 {0: 'pedestrian', 1: 'rider', 2: 'car', 3: 'truck', 4: 'bus', 5: 'train', 6: 'motorcycle', 7: 'bicycle', 8: 'traffic light', 9: 'traffic sign', 10: 'direct area', 11: 'available area'} True\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'tuple' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 11\u001B[0m\n\u001B[0;32m      9\u001B[0m stride, names, pt \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mstride, model\u001B[38;5;241m.\u001B[39mnames, model\u001B[38;5;241m.\u001B[39mpt\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(stride, names, pt)\n\u001B[1;32m---> 11\u001B[0m imgsz \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_img_size\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimgsz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# check image size\u001B[39;00m\n",
      "File \u001B[1;32mE:\\Documents\\Pycharm\\utils\\general.py:407\u001B[0m, in \u001B[0;36mcheck_img_size\u001B[1;34m(imgsz, s, floor)\u001B[0m\n\u001B[0;32m    405\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# list i.e. img_size=[640, 480]\u001B[39;00m\n\u001B[0;32m    406\u001B[0m     imgsz \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(imgsz)  \u001B[38;5;66;03m# convert to list if tuple\u001B[39;00m\n\u001B[1;32m--> 407\u001B[0m     new_size \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mmax\u001B[39m(make_divisible(x, \u001B[38;5;28mint\u001B[39m(s)), floor) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m imgsz]\n\u001B[0;32m    408\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m new_size \u001B[38;5;241m!=\u001B[39m imgsz:\n\u001B[0;32m    409\u001B[0m     LOGGER\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWARNING ⚠️ --img-size \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimgsz\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be multiple of max stride \u001B[39m\u001B[38;5;132;01m{\u001B[39;00ms\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, updating to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnew_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mE:\\Documents\\Pycharm\\utils\\general.py:407\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    405\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# list i.e. img_size=[640, 480]\u001B[39;00m\n\u001B[0;32m    406\u001B[0m     imgsz \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(imgsz)  \u001B[38;5;66;03m# convert to list if tuple\u001B[39;00m\n\u001B[1;32m--> 407\u001B[0m     new_size \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mmax\u001B[39m(\u001B[43mmake_divisible\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m, floor) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m imgsz]\n\u001B[0;32m    408\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m new_size \u001B[38;5;241m!=\u001B[39m imgsz:\n\u001B[0;32m    409\u001B[0m     LOGGER\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWARNING ⚠️ --img-size \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimgsz\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be multiple of max stride \u001B[39m\u001B[38;5;132;01m{\u001B[39;00ms\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, updating to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnew_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mE:\\Documents\\Pycharm\\utils\\general.py:655\u001B[0m, in \u001B[0;36mmake_divisible\u001B[1;34m(x, divisor)\u001B[0m\n\u001B[0;32m    653\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(divisor, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m    654\u001B[0m     divisor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(divisor\u001B[38;5;241m.\u001B[39mmax())  \u001B[38;5;66;03m# to int\u001B[39;00m\n\u001B[1;32m--> 655\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m math\u001B[38;5;241m.\u001B[39mceil(\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdivisor\u001B[49m) \u001B[38;5;241m*\u001B[39m divisor\n",
      "\u001B[1;31mTypeError\u001B[0m: unsupported operand type(s) for /: 'tuple' and 'int'"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = select_device(device)\n",
    "#print(device)\n",
    "weights=ROOT / 'ZZX12Class.pt'\n",
    "source=ROOT / 'bevdata/caltech_washington1'\n",
    "data=ROOT / 'data/bdd100k.yaml',  # dataset.yaml path\n",
    "imgsz=(480, 640),  # inference size (height, width)\n",
    "model = DetectMultiBackend(weights,device=device,dnn=False, data=data, fp16=False)\n",
    "stride, names, pt = model.stride, model.names, model.pt\n",
    "print(stride, names, pt)\n",
    "imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1  # batch_size\n",
    "dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=1)\n",
    "vid_path, vid_writer = [None] * bs, [None] * bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty(): argument 'size' failed to unpack the object at pos 3 with error \"type must be tuple of ints,but got tuple\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32mf:\\VCStudio\\yolov5\\testbevinyolo5.ipynb Cell 10\u001B[0m line \u001B[0;36m1\n\u001B[1;32m----> <a href='vscode-notebook-cell:/f%3A/VCStudio/yolov5/testbevinyolo5.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001B[0m model\u001B[39m.\u001B[39;49mwarmup(imgsz\u001B[39m=\u001B[39;49m(\u001B[39m1\u001B[39;49m, \u001B[39m3\u001B[39;49m,\u001B[39m*\u001B[39;49mimgsz))  \u001B[39m# warmup\u001B[39;00m\n",
      "File \u001B[1;32mf:\\VCStudio\\yolov5\\models\\common.py:607\u001B[0m, in \u001B[0;36mDetectMultiBackend.warmup\u001B[1;34m(self, imgsz)\u001B[0m\n\u001B[0;32m    605\u001B[0m warmup_types \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mpt, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mjit, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39monnx, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mengine, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39msaved_model, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mpb, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mtriton\n\u001B[0;32m    606\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39many\u001B[39m(warmup_types) \u001B[39mand\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdevice\u001B[39m.\u001B[39mtype \u001B[39m!=\u001B[39m \u001B[39m'\u001B[39m\u001B[39mcpu\u001B[39m\u001B[39m'\u001B[39m \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mtriton):\n\u001B[1;32m--> 607\u001B[0m     im \u001B[39m=\u001B[39m torch\u001B[39m.\u001B[39;49mempty(\u001B[39m*\u001B[39;49mimgsz, dtype\u001B[39m=\u001B[39;49mtorch\u001B[39m.\u001B[39;49mhalf \u001B[39mif\u001B[39;49;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mfp16 \u001B[39melse\u001B[39;49;00m torch\u001B[39m.\u001B[39;49mfloat, device\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mdevice)  \u001B[39m# input\u001B[39;00m\n\u001B[0;32m    608\u001B[0m     \u001B[39mfor\u001B[39;00m _ \u001B[39min\u001B[39;00m \u001B[39mrange\u001B[39m(\u001B[39m2\u001B[39m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mjit \u001B[39melse\u001B[39;00m \u001B[39m1\u001B[39m):  \u001B[39m#\u001B[39;00m\n\u001B[0;32m    609\u001B[0m         \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mforward(im)\n",
      "\u001B[1;31mTypeError\u001B[0m: empty(): argument 'size' failed to unpack the object at pos 3 with error \"type must be tuple of ints,but got tuple\""
     ]
    }
   ],
   "source": [
    "model.warmup(imgsz=(1, 3,*imgsz))  # warmup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RoboYoloW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
